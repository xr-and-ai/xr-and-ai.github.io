<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=2, user-scalable=no"
    />
    <title>CHI 2025 Workshop - Everyday AR through AI-in-the-Loop</title>
    <meta name="keywords" content="XR, AI, VR, AR, MR, Virtual Reality, Mixed Reality, Augmented Reality, Generative XR, Generative AI, LLM, ChatGPT, Context-Aware, Explainable AI" />
    <meta name="author" content="Ryo Suzuki, Mar Gonzalez-Franco, Misha Sra, and David Lindlbauer" />
    <meta name="theme-color" content="#ffffff" />
    <meta
      name="description"
      content="This workshop brings together experts and practitioners from augmented reality (AR) and artificial intelligence (AI) to shape the future of AI-in-the-loop everyday AR experiences. With recent advancements in both AR hardware and AI capabilities, we envision that everyday AR—always-available and seamlessly integrated into users’ daily environments—is becoming increasingly feasible. This workshop will explore how AI can drive such everyday AR experiences. We discuss a range of topics, including adaptive and context-aware AR, generative AR content creation, always-on AI assistants, AI-driven accessible design, and real-world-oriented AI agents. Our goal is to identify the opportunities and challenges in AI-enabled AR, focusing on creating novel AR experiences that seamlessly blend the digital and physical worlds. Through the workshop, we aim to foster collaboration, inspire future research, and build a community to advance the research field of AI-enhanced AR."
    />
    <meta property="og:title" content="CHI 2025 Workshop - Everyday AR through AI-in-the-Loop" />
    <meta property="og:description" content="This workshop brings together experts and practitioners from augmented reality (AR) and artificial intelligence (AI) to shape the future of AI-in-the-loop everyday AR experiences. With recent advancements in both AR hardware and AI capabilities, we envision that everyday AR—always-available and seamlessly integrated into users’ daily environments—is becoming increasingly feasible. This workshop will explore how AI can drive such everyday AR experiences. We discuss a range of topics, including adaptive and context-aware AR, generative AR content creation, always-on AI assistants, AI-driven accessible design, and real-world-oriented AI agents. Our goal is to identify the opportunities and challenges in AI-enabled AR, focusing on creating novel AR experiences that seamlessly blend the digital and physical worlds. Through the workshop, we aim to foster collaboration, inspire future research, and build a community to advance the research field of AI-enhanced AR." />
    <meta property="og:image" content="https://xr-and-ai.github.io/images/thumbnails/thumbnail-1.jpg" />
    <meta property="og:type" content="website" />

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css" type="text/css"/>
    <link rel="stylesheet" href="./style.css" type="text/css"/>
  </head>

  <body id="root">
    <div class="ui inverted vertical center aligned segment">
      <nav class="ui container">
        <h1 class="ui inverted header">CHI 2025</h1>
        <div class="ui borderless inverted compact menu">
          <a class="active item" href="#">Home</a>
          <a class="item" href="#organizers">Organizers</a>
          <a class="item" href="#topics">Topics</a>
          <a class="item" href="#agenda">Agenda</a>
          <a class="item" href="uist-2023">UIST 2023</a>
        </div>
      </nav>
      <div class="ui content container">
        <h2 class="ui inverted header">CHI 2025</h2>
        <h1 id="title" class="ui inverted header">Everyday AR through AI-in-the-Loop</h1>
        <h2 class="ui inverted header">Second Workshop on AR and AI</h2>
        <p>
          Saturday, April 26th, 2025<br/>
          Yokohama, Japan
        </p>
        <a class="ui huge button" href="https://forms.gle/i1Wf3GhmgoRCv68f7" target="_blank">Register Now!</a>
        <br />
        <p style="margin-top: 10px">
          Deadliine: <span style="text-decoration: line-through;">February 15th</span> <b style="color: white">February 28th, 2025</b>
        </p>
      </div>

    </div>

    <div id="cover-images" class="ui text container">
      <div class="ui link five cards">
        <a href="https://dl.acm.org/doi/10.1145/3332165.3347945" target="_blank" class="card">
          <img src="images/crop/context-aware-1.jpg">
        </a>
        <a href="https://dl.acm.org/doi/10.1145/3472749.3474803" target="_blank" class="card">
          <img src="images/crop/farapy-1.jpg">
        </a>
        <a href="https://dl.acm.org/doi/10.1145/3526113.3545702" target="_blank" class="card">
          <img src="images/crop/realitytalk-2.jpg">
        </a>
        <a href="https://dl.acm.org/doi/10.1145/3472749.3474769" target="_blank" class="card">
          <img src="images/crop/gestuar-1.jpg">
        </a>
        <a href="https://dl.acm.org/doi/10.1145/3526113.3545648" target="_blank" class="card">
          <img src="images/crop/lookhere-1.jpg">
        </a>
        <a href="https://dl.acm.org/doi/10.1145/3544548.3581449" target="_blank" class="card">
          <img src="images/crop/teachable-reality-1.jpg">
        </a>
        <a href="https://dl.acm.org/doi/10.1145/3491101.3519911" target="_blank" class="card">
          <img src="images/crop/opportunistic-interfaces-2.jpg">
        </a>
        <a href="https://dl.acm.org/doi/10.1145/3472749.3474750" target="_blank" class="card">
          <img src="images/crop/semantic-adapt-1.jpg">
        </a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3478120" target="_blank" class="card">
          <img src="images/crop/syncup-1.jpg">
        </a>
        <!--
        <a href="https://dl.acm.org/doi/10.1145/3544548.3581148" target="_blank" class="card">
          <img src="images/crop/thingshare-1.jpg">
        </a>
        <a href="https://dl.acm.org/doi/10.1145/3544548.3581566" target="_blank" class="card">
          <img src="images/crop/visual-captions-1.jpg">
        </a>
        -->
        <a href="https://dl.acm.org/doi/10.1145/3586183.3606827" target="_blank" class="card">
          <img src="images/crop/augmented-math-1.jpg">
        </a>
        <a href="https://dl.acm.org/doi/10.1145/3672539.3686784" target="_blank" class="card">
          <img src="images/crop/xdtk-1.jpg">
        </a>
        <a href="https://dl.acm.org/doi/10.1145/3379337.3415817" target="_blank" class="card">
          <img src="images/crop/wearable-subtitles-1.jpg">
        </a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3581500" target="_blank" class="card">
          <img src="images/crop/xair-2.jpg">
        </a>
        <a href="https://dl.acm.org/doi/10.1145/3379337.3415881" target="_blank" class="card">
          <img src="images/crop/depthlab-1.jpg">
        </a>
        <a href="https://ego4d-data.org/" target="_blank" class="card">
          <img src="images/crop/ego4d-3.jpg">
        </a>

        <div class="ui center aligned container" style="margin-top: 2em;">
          <a class="ui huge button" href="https://xr-and-ai.github.io/chi-2025.pdf" target="_blank">CHI Workhop PDF</a>
        </div>
      </div>
    </div>

    <div id="overview" class="ui vertical stripe segment">
      <div class="ui text container">

        <h1 id="organizers" class="ui horizontal header divider">Organizers</h1>

        <div class="ui link four cards">
          <a href="https://ryosuzuki.org/" target="_blank" class="card">
            <div class="image">
              <img src="images/organizers/ryo-suzuki.jpg">
            </div>
            <div class="content">
              <div class="header">Ryo Suzuki</div>
              <div class="description">
                University of Colorado Boulder
              </div>
            </div>
          </a>

          <a href="https://margonzalezfranco.github.io/" target="_blank" class="card">
            <div class="image">
              <img src="images/organizers/mar-gonzalez-franco.jpg">
            </div>
            <div class="content">
              <div class="header">Mar Gonzalez-Franco</div>
              <div class="description">
                Google
              </div>
            </div>
          </a>

          <a href="https://sites.cs.ucsb.edu/~sra/" target="_blank" class="card">
            <div class="image">
              <img src="images/organizers/misha-sra.jpg">
            </div>
            <div class="content">
              <div class="header">Misha Sra</div>
              <div class="description">
                UC Santa Barbara
              </div>
            </div>
          </a>

          <a href="https://www.davidlindlbauer.com/" target="_blank" class="card">
            <div class="image">
              <img src="images/organizers/david-lindlbauer.jpg">
            </div>
            <div class="content">
              <div class="header">David Lindlbauer</div>
              <div class="description">
                Carnegie Mellon University
              </div>
            </div>
          </div>
        </a>


        <h1 class="ui horizontal header divider">Program</h1>
        <!-- DAY SCHEDULE -->
        <table class="ui very compact celled striped table">
          <thead>
            <tr><th style="width:6rem">Time</th><th>Session</th><th>Details</th></tr>
          </thead>
          <tbody>
            <tr><td>09:00–09:10</td><td>Intro &amp; Welcome</td><td></td></tr>
            <tr><td>09:10–09:45</td><td>Speed‑Dating</td><td></td></tr>
            <tr><td>09:45–10:30</td><td>Keynote (Teresa Hirzle)</td><td>25 min talk + 20 min Q&amp;A</td></tr>
            <tr class="active"><td>10:30–11:10</td><td>Coffee Break</td><td></td></tr>
            <tr><td>11:10–12:30</td><td>6 Paper Talks + Activity</td>
                <td>8 min / paper + 30 min “Key take‑aways”</td></tr>
            <tr class="active"><td>12:30–14:15</td><td>Lunch</td><td></td></tr>
            <tr><td>14:15–15:20</td><td>6 Paper Talks + Activity</td>
                <td>8 min / paper + 30 min “Design curriculum”</td></tr>
            <tr class="active"><td>15:40–16:20</td><td>Break+ Buffer</td><td></td></tr>
            <tr><td>16:20–17:05</td><td>Keynote (Antti Oulasvirta)</td>
                <td>35 min talk + 10 min Q&amp;A</td></tr>
            <tr><td>17:05–17:50</td><td>Discussion</td><td>Open‑mic / Position topics</td></tr>
          </tbody>
        </table>


        <h1 class="ui horizontal header divider">Keynote Speakers</h1>
        <div class="ui stackable two column centered grid">
          <!-- — Teresa Hirzle — -->
          <div class="column">
            <a href="https://thirzle.github.io/" target="_blank" class="ui fluid rounded image">
              <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=TRtAj1wAAAAJ&citpid=1" alt="Teresa Hirzle">
            </a>
            <h3 class="ui center aligned header" style="margin-top:0.7em">
              Teresa Hirzle
              <div class="sub header">University of Copenhagen</div>
            </h3>
          </div>

          <!-- — Antti Oulasvirta — -->
          <div class="column">
            <a href="https://users.aalto.fi/~oulasvir/" target="_blank" class="ui fluid rounded image">
              <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=_z41TLwAAAAJ&citpid=7" alt="Antti Oulasvirta">
            </a>
            <h3 class="ui center aligned header" style="margin-top:0.7em">
              Antti Oulasvirta
              <div class="sub header">Aalto University</div>
            </h3>
          </div>

        </div>



        <h1 class="ui horizontal header divider">Accepted Position Papers</h1>
        <div class="ui relaxed divided list">
          <div class="item"><i class="file pdf outline icon"></i>
            <div class="content">
              <a class="header" href="chi-2025/chen-liang.pdf" target="_blank">Towards Accessible XR Hand Interactions via AI‑Supported Interaction Proxies</a>
              <div class="description">Chen Liang (U. Michigan)</div>
            </div>
          </div>
          <div class="item"><i class="file pdf outline icon"></i>
            <div class="content">
              <a class="header" href="chi-2025/eric-gonzalez.pdf" target="_blank">Practical Input Considerations for Wearable AI Assistants in XR</a>
              <div class="description">Eric J. Gonzalez (Google)</div>
            </div>
          </div>
          <div class="item"><i class="file pdf outline icon"></i>
            <div class="content">
              <a class="header" href="chi-2025/fabrice-matulic.pdf" target="_blank">Talking to Your Surroundings: Enabling Rich Human‑Object Conversations Through AI and Augmented Reality</a>
              <div class="description">Fabrice Matulic (Preferred Networks)</div>
            </div>
          </div>
          <div class="item"><i class="file pdf outline icon"></i>
            <div class="content">
              <a class="header" href="chi-2025/helen-stefanidi.pdf" target="_blank">Bringing Adaptive On‑the‑Move AR to Outdoor Environments</a>
              <div class="description">Helen Stefanidi (U. Salzburg)</div>
            </div>
          </div>
          <div class="item"><i class="file pdf outline icon"></i>
            <div class="content">
              <a class="header" href="chi-2025/hyuna-seo.pdf" target="_blank">AI‑Mediated Augmented Emotion Expression of Avatars for Social Mixed Reality</a>
              <div class="description">HyunA Seo (Seoul National University)</div>
            </div>
          </div>
          <div class="item"><i class="file pdf outline icon"></i>
            <div class="content">
              <a class="header" href="chi-2025/julian-mendez.pdf" target="_blank">ARbiter: Generating Dialogue Options and Communication Support in AR</a>
              <div class="description">Julián Méndez (TUDDresden)</div>
            </div>
          </div>
          <div class="item"><i class="file pdf outline icon"></i>
            <div class="content">
              <a class="header" href="chi-2025/mariana-fernandez-espinosa.pdf" target="_blank">Augmenting Teamwork through AI Agents as Spatial Collaborators</a>
              <div class="description">Mariana Fernandez‑Espinosa (U. NotreDame)</div>
            </div>
          </div>
          <div class="item"><i class="file pdf outline icon"></i>
            <div class="content">
              <a class="header" href="chi-2025/mark-mcgill.pdf" target="_blank">Do We Need Responsible XR? Drawing on Responsible AI to Inform Ethical Research and Practice into XRAI / the Metaverse</a>
              <div class="description">Mark McGill (U. Glasgow)</div>
            </div>
          </div>
          <div class="item"><i class="file pdf outline icon"></i>
            <div class="content">
              <a class="header" href="chi-2025/nuwan-janaka.pdf" target="_blank">Empowering Every Step: Towards Intelligent Wearable Assistants</a>
              <div class="description">Nuwan Janaka (NUS)</div>
            </div>
          </div>
          <div class="item"><i class="file pdf outline icon"></i>
            <div class="content">
              <a class="header" href="chi-2025/riku-arakawa.pdf" target="_blank">Toward Mixed-Initiative Assistance for Real-World Procedures</a>
              <div class="description">Riku Arakawa (Carnegie Mellon University)</div>
            </div>
          </div>
          <div class="item"><i class="file pdf outline icon"></i>
            <div class="content">
              <a class="header" href="chi-2025/xiaoan-liu.pdf" target="_blank">Augmenting Human Cognition through Everyday AR</a>
              <div class="description">Xiaoan (Sean) Liu (New York University)</div>
            </div>
          </div>
          <div class="item"><i class="file pdf outline icon"></i>
            <div class="content">
              <a class="header" href="chi-2025/cathy-fang.pdf" target="_blank">The Goldilocks Time Window for Proactive Interventions in Wearable AI Systems</a>
              <div class="description">Cathy Fang (MIT Media Lab)</div>
            </div>
          </div>
        </div>

        <!-- REGULAR PARTICIPANTS -->
        <h1 class="ui horizontal header divider">Participants</h1>
        <div class="ui bulleted list">
          <div class="item"><a href="https://thirzle.github.io/" target="_blank">Teresa Hirzle</a> (University of Copenhagen)</div>
          <div class="item"><a href="https://users.aalto.fi/~oulasvir/" target="_blank">Antti Oulasvirta</a> (Aalto University)</div>
          <div class="item"><a href="https://liangchenlc.com/" target="_blank">Chen Liang</a> (University of Michigan)</div>
          <div class="item"><a href="https://hci.plus/person/stefanidi/" target="_blank">Eleni (Helen) Stefanidi</a> (University of Salzburg)</div>
          <div class="item"><a href="https://www.ejgonzalez.me/" target="_blank">Eric J. Gonzalez</a> (Google LLC)</div>
          <div class="item"><a href="https://fmatulic.github.io/" target="_blank">Fabrice Matulic</a> (Preferred Networks Inc.)</div>
          <div class="item"><a href="https://hyunaseo.github.io/" target="_blank">HyunA Seo</a> (Seoul National University)</div>
          <div class="item"><a href="https://imld.de/~mendez" target="_blank">Julián Méndez</a> (TUD Dresden University of Technology)</div>
          <div class="item"><a href="https://marianafdz465.github.io/" target="_blank">Mariana Fernandez‑Espinosa</a> (University of Notre Dame)</div>
          <div class="item"><a href="https://augsoc-project.org/" target="_blank">Mark McGill</a> (University of Glasgow)</div>
          <div class="item"><a href="https://rikky0611.github.io/" target="_blank">Riku Arakawa</a> (Carnegie Mellon University)</div>
          <div class="item"><a href="https://seanliu.io/" target="_blank">Xiaoan Liu</a> (New York University)</div>
          <div class="item"><a href="https://nuwanjanaka.info" target="_blank">Nuwan Janaka</a> (National University of Singapore)</div>
          <div class="item"><a href="https://cathy-fang.com/" target="_blank">Cathy Fang</a> (MIT Media Lab)</div>
          <div class="item"><a href="https://marc-satkowski.de" target="_blank">Marc Satkowski</a> (TUD Dresden University of Technology)</div>
          <div class="item"><a href="https://www.hyeyoungjo.com/" target="_blank">Hye‑Young Jo</a> (University of Colorado Boulder)</div>
          <div class="item"><a href="https://www.hyunchul.com/" target="_blank">Hyunchul Lim</a> (Cornell University)</div>
          <div class="item"><a href="https://jiahaoli.net/" target="_blank">Jiahao Nick Li</a> (Apple)</div>
          <div class="item"><a href="https://people.cs.nycu.edu.tw/~liweichan/" target="_blank">Liwei Chan</a> (National Yang Ming Chiao Tung University)</div>
          <div class="item"><a href="https://nels.dev" target="_blank">Nels Numan</a> (University College London / Google)</div>
          <div class="item"><a href="https://www.radhakumaran.com/" target="_blank">Radha Kumaran</a> (University of California, Santa Barbara)</div>
          <div class="item"><a href="https://szollmann.github.io" target="_blank">Stefanie Zollmann</a> (Aarhus University)</div>
          <div class="item"><a href="https://www.flomue.com/" target="_blank">Florian Müller</a> (Technical University of Darmstadt)</div>
          <div class="item"><a href="http://barpit20.github.io/" target="_blank">Arpit Bhatia</a> (University of Copenhagen)</div>
          <div class="item"><a href="https://www.duotun-wang.co.uk/" target="_blank">Duotun Wang</a> (HKUST – Guangzhou)</div>
          <div class="item"><a href="https://cs.umd.edu/~gsunlee" target="_blank">Geonsun Lee</a> (University of Maryland, College Park)</div>
          <div class="item"><a href="https://julian-rasch.com" target="_blank">Julian Rasch</a> (Ludwig‑Maximilians‑Universität München)</div>
          <div class="item"><a href="https://riccardobovo.com/" target="_blank">Riccardo Bovo</a> (Imperial College London)</div>
          <div class="item"><a href="https://wallacewangchao.github.io/" target="_blank">Chao Wang</a> (Honda Research Institute Europe)</div>
          <div class="item"><a href="https://esenka.co/" target="_blank">Esen Tütüncü</a> (University of Barcelona)</div>
          <div class="item"><a href="https://ajingu.github.io/" target="_blank">Arata Jingu</a> (Saarland University)</div>
          <div class="item">Xueyang Wang (Tsinghua University)</div>
          <div class="item">Sieun Park (Seoul National University)</div>
          <div class="item">Harald Reiterer (University of Konstanz)</div>
          <div class="item">Bonnie Wu (University of Calgary)</div>
        </div>


        <h1 class="ui horizontal header divider">Overview</h1>
        <p>Augmented Reality (AR) technologies are continuously advancing, both in terms of hardware and software. Smaller form factors, extended battery life, and constant connectivity make it possible to results in a paradigm shift in how we think about AR: while current AR scenarios are focused on specialized applications such as productivity or maintenance, we believe that <b>everyday AR</b> is increasingly feasible. With everyday AR we refer to the <b>making AR always available to users</b>, enabling seamless interactions with the digital world, and potentially replacing or augmenting currently predominant technologies such as smartphone or desktop computing. Users will be able to ubiquitously query and interact with digital information, communicate with other users and virtual agents, and leverage AR for a majority of their interactions.</p>
        <p>This rise of everyday AR is fueled not just by improvements in hardware and software infrastructure, but is also heavily tied to recent advances in Artificial Intelligence (AI) and Machine Learning (ML). Generative AI techniques, for example, enable the creation of multi-modal digital content on-the-fly; and Large Language Models lead to advancements in various applications domains and make it feasible for users to interact with virtual agents through text and other inputs in a "natural" way. Leveraging advances in those areas will enable researchers and practitioners to develop new and refine existing interactions for AR. We believe that in order to make everyday AR feasible, we need to adopt an <b>AI-in-the-loop</b> approach, where the digital interactions and content continuously anticipate and adapt to users every-changing needs and context. Thus, we believe the combination of AR and AI will enable us to move beyond monolithic applications in AR towards a truly user-centered and adaptive experience where both scene understanding and content generation becomes dynamic.</p>
        <p>The goal of this workshop, which is an evolution of <a href="uist-2023"><b>our previous XR and AI Workshop at ACM UIST 2023</b></a>, is to bridge the fields of AR and AI and enable discussions around the feasibility and requirements for AI-enabled everyday AR. Our previous workshop was broadly centered around the topic of what is possible when AR and AI are combined, the second iteration focuses on everyday AR interactions. We plan to explore usage scenarios of this new paradigm, explore requirements and limitations of current and future AR with AI-in-the-loop, and identify and address common challenges of AR (e.g., accessibility, privacy) and AI (e.g., explainability, sustainability). In that sense, we consider this workshop as a natural evolution, not only we meet but also define a way forwards for the community. This workshop seeks to further refine the thinking of the community towards a shared vision, and should lay the foundation for future workshops at conferences, longer seminars, and work towards a reference paper to consolidate the research efforts. As such we propose a workshop very oriented to shared vision, and make a call for participation in which we ask participants to share how they imagine the future of AR as we move to a world where AR is to AI what screens has been to computers.</p>
        <a class="ui large button" href="https://xr-and-ai.github.io/chi-2025.pdf" target="_blank">Read the Proposal PDF</a>

        <h1 id="topics" class="ui horizontal header divider">Call for Participation</h1>

        <table class="ui celled table">
          <thead>
            <tr>
              <th>Category</th>
              <th>Details</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Application Requirements</td>
              <td>
                Complete an <a href="https://forms.gle/i1Wf3GhmgoRCv68f7" target="_blank">application form</a> (background, experience, and interests in AR/AI)<br>
                Optional: Submit a position paper
              </td>
            </tr>
            <tr>
              <td>Submission Format</td>
              <td>Up to 2 pages, ACM double-column format, without references (for optional position papers)</td>
            </tr>
            <tr>
              <td>Application Deadline</td>
              <td>
                <!-- February 15th, 2025 -->
                <span style="text-decoration: line-through;">February 15th</span>
                <span style="background-color: yellow;">February 28th, 2025</span>
              </td>
            </tr>
            <tr>
              <td>Participant Selection</td>
              <td>
                Based on relevance of research/background and diversity of participants
              </td>
            </tr>
            <tr>
              <td>Attendance Requirements</td>
              <td>
                Participants must attend in person in Yokohama and register for both the workshop and at least one day of the CHI 2025 conference
              </td>
            </tr>
          </tbody>
        </table>

        <div class="ui center aligned container" style="margin-top: 2em;">
          <a class="ui huge button" href="https://forms.gle/i1Wf3GhmgoRCv68f7" target="_blank">Application Form</a>
        </div>
        <br/>

        <p>This one-day, in-person CHI 2025 workshop invites researchers, practitioners, and students interested in exploring the future of AI-enabled augmented reality (AR) to join us in shaping a vision for "Everyday AR." We aim to create a shared vocabulary and research agenda for AI-driven AR, inspiring new directions and fostering future collaborations. <b>To apply for participation, interested individuals should complete an <a href="https://forms.gle/i1Wf3GhmgoRCv68f7" target="_blank"><b>application form</b></a> indicating their background, experience, and interests in AR and AI by <span style="text-decoration: line-through;">February 15th</span> <span style="background-color: yellow;">February 28th, 2025</span></b>. Applicants are encouraged to optionally submit a position paper of up to 2 pages (without references) in the ACM double-column format. Participants will be selected based on their application materials, with a focus on the relevance of their research or disciplinary background to the workshop topic, and a goal of creating a diverse and academically engaging environment.</p>

        <p>Note that per CHI 2025 policies, workshop participants (including at least one author of each position paper, for applications that optionally include position papers) must attend the workshop in Yokohama, and all participants must register for both the workshop and for at least one day of the conference.</p>

        <p>One of the primary goals is to foster a strong community around this emerging topic, leveraging the networking opportunities provided by face-to-face interactions, such as a group lunch and dinner. To ensure an intimate and engaging workshop experience, we plan to cap attendance at about 30 participants.</p>

        <h1 id="topics" class="ui horizontal header divider">Topics of Interest</h1>

        <p>This workshop welcomes HCI researchers and practitioners in AR, AI, machine learning, and computational interaction domains to share diverse perspectives and expertise. There are several domains that are not fully explored yet in the literature of XR and AI. We plan to discuss the topics that include but are not limited to the following areas:</p>

        <p class="ui bulleted list">
          <p class="item"><i><b>Adaptive and Context-Aware AR:</b><br/>
          Unlike traditional interfaces on smartphones or computers, AR can embed virtual elements into the physical world. However, poorly designed AR interfaces risk overwhelming or distracting users. Context-aware AR addresses this by adapting virtual elements based on users' needs, context, and environment. Recent research has explored blending AR interfaces with physical objects using computational methods, leveraging everyday objects for virtual interactions, and applying adaptive AR for use cases like improving accessibility for people with low vision. We aim to expand this research into broader everyday applications by using advanced AI capabilities. These can enable AR systems to better understand contextual information, such as room geometry, physical object affordances, and user activities, creating more seamless and adaptive experiences.</i></p>

          <p class="item"><i><b>Always-on AI Assistant by Integrating LLMs and AR:</b><br/>
          Large Language Models (LLMs) can enhance AR by enabling always-on AI assistants that seamlessly support user activities and blend into the environment. Unlike screen-based interactions, LLM-AR integration allows intuitive, multimodal interactions with digital content. Recent work demonstrates augmented object intelligence, gaze-based question answering, and wearable-based assistance. We aim to advance these capabilities by leveraging LLMs' multimodal potential, combining visual, tangible, and spatial modalities unique to AR interfaces.</i></p>

          <p class="item"><i><b>AI-Assisted Task Guidance in AR:</b><br/>
          AI-assisted task guidance offers significant advantages over traditional video-based instruction, which provides static, one-way demonstrations. AI systems can analyze users’ movements in real-time, delivering personalized feedback on form, timing, and technique. This interactive approach has the potential to accelerate learning, prevent bad habits, and improve performance. Beyond full-body activities like fitness or sports, we are also interested in manual tasks such as cooking, repairs, or assembly. Additionally, we aim to explore how AI systems can adapt to user progress, gradually increasing task complexity and offering encouragement for a more engaging and effective learning experience.</i></p>

          <p class="item"><i><b>AI-in-the-Loop On-Demand AR Content Creation:</b><br/>
          Generative AR can revolutionize the creation of interactive AR content by leveraging advances in generative AI. Techniques like GANs and Stable Diffusion enable real-time content generation, 3D animation, and immersive environment design. Beyond static image generation, generative AI can create interactive AR content using LLMs’ code generation capabilities and by making real-world objects interactive. We aim to explore how generative AI can utilize AR’s unique spatial aspects, such as 3D scene understanding and spatial interactions, to generate richer content—text, visuals, motion, video, and 3D objects. We also seek to address the challenges and opportunities of integrating these capabilities into AR environments.</i></p>

          <p class="item"><i><b>AI for Accessible AR Design:</b><br/>
          AI has the potential to enhance accessibility in AR design and experiences, addressing barriers faced by individuals with hand-motor impairments. Traditional input methods, such as bimanual keyboard-mouse setups in tools like Unity or handheld controllers in XR experiences, exclude these users. To overcome this, we propose exploring AI-driven alternative inputs, including voice commands, gaze tracking, facial expression recognition, and subtle body movements interpreted by AI. Additionally, AI can adapt content difficulty, interfaces, and narratives to individual capabilities. This inclusive approach not only expands access to AR design and experiences but also innovates interaction paradigms, transforming how virtual worlds are created and engaged with.</i></p>

          <p class="item"><i><b>Real-World-Oriented AI Agents:</b><br/>
          We are interested in designing AI agents that enhance the physical world by bridging the digital and physical realms. These agents would understand spatial relationships, object affordances, and user activities to assist with navigation, support complex tasks, and provide personalized guidance. Recent advancements, such as AI agents that guide users’ attention through mixed reality avatars, demonstrate promising potential. We aim to explore how AI agents can be designed to be aware of and responsive to real-world contexts, enabling more intuitive and adaptive AR experiences.</i></p>
        </p>

        <h1 id="agenda" class="ui horizontal header divider">Agenda</h1>

        <p class="ui bulleted list">

          <p class="item"><i><b>Introductions and Lightning Talks:</b><br/>
          The organizers will kick off the workshop with introductions and each participant's lighting talks highlighting their related research experience. To start community building, we will facilitate pre-workshop engagement to cultivate and propose an initial set of themes through the creation of a shared slide deck that highlights each participant's background and interests.</i></p>

          <p class="item"><i><b>Position Paper Presentations:</b><br/>
          In this workshop, we will announce a public call for position papers from the CHI community, inviting researchers and practitioners to contribute their insights. The purpose of these talks is to provide an opportunity for attendees to share their opinions and positions on their recent and ongoing work related to AR and AI. We aim to foster a collaborative environment where participants can present radical ideas, exchange feedback, and inspire new directions for research. For each accepted position paper, we plan to allocate 15-20 minutes for presentations. Additionally, with the consent of the participants, we plan to publish these position papers after the workshop to make the insights available to a broader audience and promote further discussion.</i></p>

          <p class="item"><i><b>Keynote Talks:</b><br/>
          We will also invite distinguished speakers to present keynote talks, with the goal of sharing forward-thinking visions and ideas with the audience. These talks are intended to inspire and set future directions for the field, similar to the UIST Vision Talks. Each keynote session will be approximately one hour long, providing ample time for in-depth exploration of ideas. We also plan to publish the title, speaker bio, and abstract on the workshop website to provide attendees with context and background information ahead of the event.</i></p>

          <p class="item"><i><b>Participant Activities:</b><br/>
          Our workshop aims to provide a balanced mix of presentations and interactive participant activities to foster engagement and collaboration among attendees. To achieve this, we have organized several activities designed to encourage hands-on exploration and creative problem-solving. These include: 1) Hackathon and Rapid Prototyping: Teams use low-fidelity materials (paper, cardboard, markers) to mock up an XR interface or device. Teams incorporate how AI might enhance the user experience. Teams present prototypes in a "science fair" style showcase. 2) Design Sprint: We will challenge participants to redesign a common XR experience for users with specific disabilities. Users may integrate AI to propose adaptive interfaces or alternative input methods. 3) XR-AI Mind Mapping: We will create a large, shared mind map of potential XR+AI applications across industries and encourage participants to add ideas throughout the day.</i></p>

          <p class="item"><i><b>Theme Organization and Discussion:</b><br/>
          Participants will collectively extrapolate themes from the presentations and shared readings. The discussed themes will be combined with the initial set of topics developed prior to the workshop to serve as guiding directions for the first round of discussions. Once a preliminary list of discussion topics has been defined, each topic will be assigned a ‘table.’ During the session, participants will rotate between tables to engage in focused discussions of topics of their choice. One participant at each table will be designated as the discussion mediator, whose responsibilities will involve guiding and documenting the discussion. The second session on theme organization will begin with lightning talks by the discussion mediators summarizing earlier conversations. Participants will then collectively revisit the discussion topics, reorganizing accordingly based on the results of the first session and expert perspectives. With the refined list of topics, the remainder of the session will follow the same format as the first round of discussions.</i></p>

          <p class="item"><i><b>Defining Future Challenges and Research Directions:</b><br/>
          In this final discussion session, we will begin by regrouping and refining the list of discussion topics based on the results of the discussion sessions. The final workshop session will focus on summarizing the workshop findings and defining next steps. First, the organizers will provide a recap of the workshop activities, including the defined themes from the pre-workshop activities and a summary of the morning and afternoon discussion outcomes. The floor will then be opened for participants to contribute their reflections on the workshop discussion. A final discussion will be held around potential future directions, such as follow-up workshops and publications.</i></p>

        </p>

        <!--
        <table class="ui basic table">
          <thead>
            <tr>
              <th>Time</th>
              <th>Activity</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>09:00 - </td>
              <td>Welcome & Social ice-breakers</td>
            </tr>
            <tr>
              <td>09:30 - </td>
              <td>Keynote Talk</td>
            </tr>
            <tr>
              <td>10:30 - </td>
              <td>Lightining Talks by Participants</td>
            </tr>
            <tr>
              <td>11:30 - </td>
              <td>Small Group Discussion based on Chosen Topics</td>
            </tr>
            <tr>
              <td>12:30</td>
              <td>Lunch break</td>
            </tr>
            <tr>
              <td>13:30 - </td>
              <td>Small Group Discussion based on Chosen Topics</td>
            </tr>
            <tr>
              <td>16:00 - </td>
              <td>Large Group Discussion and Synthesis</td>
            </tr>
            <tr>
              <td>17:00</td>
              <td>Closing Discussion</td>
            </tr>
          </tbody>
        </table>
        -->

        <h1 id="contact" class="ui horizontal header divider">Acknowledgements</h1>
        <p>This workshop was partially funded by the JST PRESTO Grant Number JPMJPR23I5.</p>

        <h1 id="contact" class="ui horizontal header divider">Questions</h1>
        <p>If you have any questions, feel free to <a href="mailto:ryo.suzuki@colorado.edu" target="_blank">contact us</a>.</p>


      </div>
    </div>
  </div>

  </body>
</html>